{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import statistics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/home/dona/bm75/Project_Chathurika/Classification/data/revised_data/\"\n",
    "path_for_models = BASE + \"remove_background/models/\"\n",
    "path_to_results = BASE + \"remove_background/results/\"\n",
    "segments = ['body', 'head', 'thorax', 'abdomen']\n",
    "fold = 5 #number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_to_results, segment, phase):\n",
    "    \n",
    "    output_files = []\n",
    "    data = []\n",
    "    \n",
    "    files = os.listdir(path_to_results)\n",
    "    files.sort()\n",
    "    \n",
    "    if phase == \"test\":\n",
    "        for file in files:\n",
    "            if file[-8:-4] == \"test\" and (segment in file.lower()):\n",
    "                output_files.append(file)\n",
    "    elif phase == \"train\":\n",
    "        for file in files:\n",
    "            if file[-9:-4] == \"train\" and (segment in file.lower()):\n",
    "                output_files.append(file)\n",
    "        \n",
    "    no_of_files = len(output_files)\n",
    "    #print(\"total_files\", no_of_files)\n",
    "    \n",
    "    for i, file in enumerate(output_files):\n",
    "        parameters = file.split('_')\n",
    "        fold = parameters[1]\n",
    "        segment = parameters[0]\n",
    "        phase = parameters[2]\n",
    "        f = open(path_to_results + file)\n",
    "        f.readline()\n",
    "        \n",
    "        predicted_label = []\n",
    "        predicted_values = []\n",
    "        actual_label = []\n",
    "        sample_id = []\n",
    "        \n",
    "        for line in f:\n",
    "            splits = line.strip().split(',')\n",
    "            preds = [float(splits[0]), float(splits[1])]\n",
    "            predicted_values.append(preds)\n",
    "            predicted_label.append(softmax(np.array(preds))[1])\n",
    "            actual_label.append(int(splits[2]))\n",
    "            sample_id.append(int(splits[3]))\n",
    "\n",
    "        obj = { 'fold': fold,\n",
    "                'predicted_label' : predicted_label,\n",
    "                'predicted_value' : predicted_values,\n",
    "                'actual_label': actual_label,\n",
    "                'sample_id' : sample_id\n",
    "              }\n",
    "        data.append(obj)\n",
    "        \n",
    "    sorted_data = sorted(data, key=lambda d: d['fold'])\n",
    "        \n",
    "    return sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predicted_labels, segment = \"\"):\n",
    "    acc = accuracy_score(labels, predicted_labels)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, predicted_labels, average=\"binary\")\n",
    "    #print('accuracy: {}, precision: {}, recall: {}, f1-score: {}'.format(acc, p, r, f))\n",
    "    return acc, p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance for indivial body part\n",
    "for segment in segments:\n",
    "    data = read_data(path_to_results, segment, 'test')\n",
    "    #print(data)\n",
    "    acc_list = []\n",
    "    p_list = []\n",
    "    r_list = []\n",
    "    f1_list = []\n",
    "    print(\"segment: \", segment)\n",
    "    for d in data:\n",
    "        #print('fold: ', d['fold'])\n",
    "        actual_labels = np.array(d['actual_label']).reshape(-1, 1)\n",
    "        predicted_labels = d['predicted_label']\n",
    "        predicted_labels = np.array(predicted_labels).reshape(-1,1)\n",
    "        predicted_labels[predicted_labels>0.5]=1\n",
    "        predicted_labels[predicted_labels<=0.5]=0\n",
    "        acc, p, r, f = confusion_matrix(actual_labels, predicted_labels, segment)\n",
    "        acc_list.append(acc)\n",
    "        p_list.append(p)\n",
    "        r_list.append(r)\n",
    "        f1_list.append(f) \n",
    "    mean = [statistics.mean(acc_list), statistics.mean(p_list), statistics.mean(r_list), statistics.mean(f1_list)]\n",
    "    stdev = [statistics.stdev(acc_list), statistics.stdev(p_list), statistics.stdev(r_list), statistics.stdev(f1_list)]\n",
    "    print('Mean values - accuracy: {}, precision: {}, recall: {}, f1-score: {}'.format(mean[0], mean[1], mean[2], mean[3]))\n",
    "    print('Stdv values - accuracy: {}, precision: {}, recall: {}, f1-score: {}'.format(stdev[0], stdev[1], stdev[2], stdev[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_test = read_data(path_to_results, 'head', 'test')\n",
    "head_train = read_data(path_to_results, 'head', 'train')\n",
    "thorax_test = read_data(path_to_results, 'thorax', 'test')\n",
    "thorax_train = read_data(path_to_results, 'thorax', 'train')\n",
    "abdomen_test = read_data(path_to_results, 'abdomen', 'test')\n",
    "abdomen_train = read_data(path_to_results, 'abdomen', 'train')\n",
    "body_test = read_data(path_to_results, 'body', 'test')\n",
    "body_train = read_data(path_to_results, 'body', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_preds(d):\n",
    "    predicted_labels = d['predicted_label']\n",
    "    predicted_labels = np.array(predicted_labels).reshape(-1,1)\n",
    "    predicted_labels[predicted_labels>0.5]=1\n",
    "    predicted_labels[predicted_labels<=0.5]=0\n",
    "    return predicted_labels\n",
    "\n",
    "def plot_model_scores(model, is_needed = False):\n",
    "    if is_needed:\n",
    "        objects = ('body', 'head', 'thorax', 'abdomen')\n",
    "        y_pos = np.arange(len(objects))\n",
    "        #print(model.coef_)\n",
    "        plt.bar(y_pos, model.coef_[0], align='center', alpha=0.5)\n",
    "        plt.xticks(y_pos, objects)\n",
    "        plt.show()\n",
    "\n",
    "def train_model(train_x, train_y, test_x, text_y):\n",
    "    model = LogisticRegression().fit(train_x, train_y)\n",
    "    file = path_to_results + 'ensembling_model.sav' #uncomment if you want to save the model\n",
    "    pickle.dump(model, open(file, 'wb'))\n",
    "    ensemble_train_preds = model.predict(train_x)\n",
    "    ensemble_test_preds = model.predict(test_x)\n",
    "    plot_model_scores(model, True)\n",
    "    return ensemble_train_preds, ensemble_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance for the augmented model\n",
    "acc_list = []\n",
    "p_list = []\n",
    "r_list = []\n",
    "f1_list = []\n",
    "for i in range(0, fold):\n",
    "    train_x = np.concatenate((process_preds(body_train[i]), process_preds(head_train[i]), process_preds(thorax_train[i]), process_preds(abdomen_train[i])),axis = -1) \n",
    "    train_y = np.array(body_train[i]['actual_label'])\n",
    "    test_x = np.concatenate((process_preds(body_test[i]), process_preds(head_test[i]), process_preds(thorax_test[i]), process_preds(abdomen_test[i])),axis = -1) \n",
    "    test_y = np.array(body_test[i]['actual_label'])\n",
    "    ensemble_train_preds, ensemble_test_preds = train_model(train_x, train_y, test_x, test_y)\n",
    "    acc, p, r, f = confusion_matrix(test_y, ensemble_test_preds, 'ensamble_test')\n",
    "    acc_list.append(acc)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "    f1_list.append(f)\n",
    "mean = [statistics.mean(acc_list), statistics.mean(p_list), statistics.mean(r_list), statistics.mean(f1_list)]\n",
    "stdev = [statistics.stdev(acc_list), statistics.stdev(p_list), statistics.stdev(r_list), statistics.stdev(f1_list)]\n",
    "print('Mean values - accuracy: {}, precision: {}, recall: {}, f1-score: {}'.format(mean[0], mean[1], mean[2], mean[3]))\n",
    "print('Stdv values - accuracy: {}, precision: {}, recall: {}, f1-score: {}'.format(stdev[0], stdev[1], stdev[2], stdev[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
